"""
voicevox_srt_generator_fixed_structure.py

NextStage Gaming ãƒãƒ£ãƒ³ãƒãƒ«å°‚ç”¨ VOICEVOX-SRTçµ±åˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆæ§‹é€ ä¿®æ­£ç‰ˆï¼‰
VOICEVOXã§ç”Ÿæˆã—ãŸvvprojãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Œå…¨åŒæœŸãƒ»ç²¾å¯†å­—å¹•åˆ†å‰²å¯¾å¿œSRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›

ä¿®æ­£å†…å®¹:
- VVPROJãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®æ­£ã—ã„ç†è§£ï¼ˆaudioKeys: ãƒªã‚¹ãƒˆ, audioItems: è¾æ›¸ï¼‰
- æ­£ç¢ºãªå†ç”Ÿé †åºã®ä¿æŒï¼ˆaudioKeysã®é †åºï¼‰
- é€£ç¶šæ™‚é–“è»¸è¨ˆç®—ï¼ˆç©ºç™½ãªã—ï¼‰
- voicevox_srt_generator.py ã®å„ªã‚ŒãŸæ™‚é–“è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶™æ‰¿

Author: AI Assistant (Fixed structure handling)
For: NextStage Gaming ãƒãƒ£ãƒ³ãƒãƒ« Street Fighter 6å®Ÿæ³å‹•ç”»ç·¨é›†åŠ¹ç‡åŒ–
Version: 3.0 - Structure Fixed
"""

import json
import math
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import numpy as np

# fugashi (MeCab wrapper) for natural Japanese segmentation
try:
    from fugashi import GenericTagger
    MECAB_AVAILABLE = True
except ImportError:
    print("Warning: fugashi not available. Natural segmentation will be limited.")
    MECAB_AVAILABLE = False

# VOICEVOX official constants
FRAMERATE = 93.75  # 24000 / 256 [frame/sec] - VOICEVOX official framerate
DEFAULT_SAMPLING_RATE = 24000

# Configuration constants
MAX_CHARS = 26  # åŸºæœ¬æ–‡å­—æ•°åˆ¶é™ï¼ˆæ„Ÿæƒ…è¡¨ç¾ã¯ä¾‹å¤–ï¼‰
MAX_LINES = 2   # å³å¯†ãªè¡Œæ•°åˆ¶é™

@dataclass
class Mora:
    """VOICEVOXå…¬å¼æº–æ‹ ã®Moraãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    text: str
    consonant: Optional[str] = None
    consonant_length: Optional[float] = None
    vowel: str = ""
    vowel_length: float = 0.0
    pitch: float = 0.0

@dataclass
class AccentPhrase:
    """VOICEVOXå…¬å¼æº–æ‹ ã®AccentPhraseãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    moras: List[Mora] = field(default_factory=list)
    accent: int = 1
    pause_mora: Optional[Mora] = None
    is_interrogative: bool = False

@dataclass
class AudioQuery:
    """VOICEVOXå…¬å¼æº–æ‹ ã®AudioQueryãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    accent_phrases: List[AccentPhrase] = field(default_factory=list)
    speedScale: float = 1.0
    pitchScale: float = 0.0
    intonationScale: float = 1.0
    volumeScale: float = 1.0
    prePhonemeLength: float = 0.1
    postPhonemeLength: float = 0.1
    pauseLength: Optional[float] = None
    pauseLengthScale: float = 1.0
    outputSamplingRate: int = DEFAULT_SAMPLING_RATE
    outputStereo: bool = False
    kana: Optional[str] = None

@dataclass
class SRTEntry:
    """SRTå­—å¹•ã‚¨ãƒ³ãƒˆãƒª"""
    index: int
    start_time: str
    end_time: str
    text: str

class VOICEVOXOfficialCalculator:
    """VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªæ™‚é–“è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def _to_frame(sec: float) -> int:
        """VOICEVOXå…¬å¼ã®ç§’â†’ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›"""
        sec_rounded = np.round(sec * FRAMERATE)
        return int(sec_rounded)
    
    @staticmethod
    def _generate_silence_mora(length: float) -> Mora:
        """éŸ³ã®é•·ã•ã‚’æŒ‡å®šã—ã¦ç„¡éŸ³ãƒ¢ãƒ¼ãƒ©ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        return Mora(text="ã€€", vowel="sil", vowel_length=length, pitch=0.0)
    
    @staticmethod
    def _apply_prepost_silence(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤å‰å¾Œç„¡éŸ³ã‚’ä»˜åŠ ã™ã‚‹"""
        pre_silence_moras = [VOICEVOXOfficialCalculator._generate_silence_mora(query.prePhonemeLength)]
        post_silence_moras = [VOICEVOXOfficialCalculator._generate_silence_mora(query.postPhonemeLength)]
        return pre_silence_moras + moras + post_silence_moras
    
    @staticmethod
    def _apply_pause_length(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤ç„¡éŸ³æ™‚é–“ã‚’é©ç”¨ã™ã‚‹"""
        if query.pauseLength is not None:
            for mora in moras:
                if mora.vowel == "pau":
                    mora.vowel_length = query.pauseLength
        return moras
    
    @staticmethod
    def _apply_pause_length_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤ç„¡éŸ³æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            if mora.vowel == "pau":
                mora.vowel_length *= query.pauseLengthScale
        return moras
    
    @staticmethod
    def _apply_speed_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤è©±é€Ÿã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            mora.vowel_length /= query.speedScale
            if mora.consonant_length:
                mora.consonant_length /= query.speedScale
        return moras
    
    @staticmethod
    def _apply_pitch_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤éŸ³é«˜ã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            mora.pitch *= 2**query.pitchScale
        return moras
    
    @staticmethod
    def _apply_intonation_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤æŠ‘æšã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        # æœ‰å£°éŸ³ç´  (f0>0) ã®å¹³å‡å€¤ã«å¯¾ã™ã‚‹ä¹–é›¢åº¦ã‚’ã‚¹ã‚±ãƒ¼ãƒ«
        voiced = [mora for mora in moras if mora.pitch > 0]
        if voiced:
            mean_f0 = np.mean([mora.pitch for mora in voiced])
            if not math.isnan(mean_f0):
                for mora in voiced:
                    mora.pitch = (mora.pitch - mean_f0) * query.intonationScale + mean_f0
        return moras
    
    @staticmethod
    def _count_frame_per_unit(moras: List[Mora]) -> Tuple[np.ndarray, np.ndarray]:
        """éŸ³ç´ ã‚ãŸã‚Šãƒ»ãƒ¢ãƒ¼ãƒ©ã‚ãŸã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ é•·ã‚’ç®—å‡ºã™ã‚‹"""
        frame_per_phoneme = []
        frame_per_mora = []
        
        for mora in moras:
            vowel_frames = VOICEVOXOfficialCalculator._to_frame(mora.vowel_length)
            consonant_frames = (
                VOICEVOXOfficialCalculator._to_frame(mora.consonant_length) 
                if mora.consonant_length is not None else 0
            )
            mora_frames = vowel_frames + consonant_frames
            
            if mora.consonant:
                frame_per_phoneme.append(consonant_frames)
            frame_per_phoneme.append(vowel_frames)
            frame_per_mora.append(mora_frames)
        
        return np.array(frame_per_phoneme, dtype=np.int64), np.array(frame_per_mora, dtype=np.int64)
    
    @staticmethod
    def _apply_voicevox_processing_pipeline(moras: List[Mora], query: AudioQuery, include_prepost_silence: bool = True) -> List[Mora]:
        """VOICEVOXå…¬å¼å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        if include_prepost_silence:
            moras = VOICEVOXOfficialCalculator._apply_prepost_silence(moras, query)
        
        moras = VOICEVOXOfficialCalculator._apply_pause_length(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_pause_length_scale(moras, query)  
        moras = VOICEVOXOfficialCalculator._apply_speed_scale(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_pitch_scale(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_intonation_scale(moras, query)
        
        return moras
    
    @staticmethod
    def calculate_accurate_duration(query: AudioQuery) -> float:
        """VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªéŸ³å£°æ™‚é–“è¨ˆç®—"""
        # ã‚¢ã‚¯ã‚»ãƒ³ãƒˆå¥ç³»åˆ—ã‹ã‚‰ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã‚’æŠ½å‡º
        moras = []
        for accent_phrase in query.accent_phrases:
            moras.extend(accent_phrase.moras)
            if accent_phrase.pause_mora:
                moras.append(accent_phrase.pause_mora)
        
        # VOICEVOXå…¬å¼å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é©ç”¨ï¼ˆå‰å¾Œç„¡éŸ³è¾¼ã¿ï¼‰
        processed_moras = VOICEVOXOfficialCalculator._apply_voicevox_processing_pipeline(
            moras, query, include_prepost_silence=True
        )
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§ã®è¨ˆç®—
        _, frame_per_mora = VOICEVOXOfficialCalculator._count_frame_per_unit(processed_moras)
        
        total_frame = frame_per_mora.sum()
        return total_frame / FRAMERATE

class AdvancedSegmentSplitter:
    """é«˜åº¦ãªå­—å¹•åˆ†å‰²ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.tagger = None
        if MECAB_AVAILABLE:
            try:
                self.tagger = GenericTagger()
            except Exception:
                pass
    
    def split_text_smart(self, text: str, max_chars: int = MAX_CHARS) -> List[str]:
        """è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„å­—å¹•åˆ†å‰²"""
        if len(text) <= max_chars:
            return [text]
        
        # æ„Ÿæƒ…è¡¨ç¾ã®ä¿è­·
        emotional_parts = []
        text_cleaned = text
        
        # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ãƒ»æ„Ÿå˜†è©ã‚’ä¿è­·
        emotion_pattern = r'([ï¼ï¼Ÿã€‚ã€]+|[ï¼ï¼Ÿ]{2,}|[ã€‚]{2,}|[ã€]{2,})'
        emotions = re.findall(emotion_pattern, text)
        for i, emotion in enumerate(emotions):
            placeholder = f"<EMO{i}>"
            text_cleaned = text_cleaned.replace(emotion, placeholder, 1)
            emotional_parts.append(emotion)
        
        # åˆ†å‰²å€™è£œç‚¹ã‚’ç‰¹å®š
        split_candidates = []
        
        # è‡ªç„¶ãªåŒºåˆ‡ã‚Šç‚¹
        natural_breaks = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ãŒã€', 'ã§ã€', 'ã¦ã€', 'ã—ã€', 'ã®ã§ã€', 'ã‹ã‚‰ã€']
        for break_point in natural_breaks:
            for match in re.finditer(re.escape(break_point), text_cleaned):
                split_candidates.append((match.end(), len(break_point)))
        
        # MeCabã«ã‚ˆã‚‹å½¢æ…‹ç´ è§£æåˆ†å‰²ç‚¹
        if self.tagger:
            try:
                pos = 0
                for word in self.tagger(text_cleaned):
                    pos += len(str(word).split('\t')[0])
                    # å‹•è©ã€åŠ©è©ã®å¾Œã¯åˆ†å‰²ã—ã‚„ã™ã„
                    if any(feature in str(word) for feature in ['å‹•è©', 'åŠ©è©']):
                        split_candidates.append((pos, 1))
            except Exception:
                pass
        
        # æœ€é©ãªåˆ†å‰²ç‚¹ã‚’é¸æŠ
        split_candidates = sorted(set(split_candidates), key=lambda x: x[0])
        
        segments = []
        start = 0
        
        for pos, _ in split_candidates:
            if pos > start and pos - start <= max_chars:
                segment = text_cleaned[start:pos].strip()
                if segment:
                    # æ„Ÿæƒ…è¡¨ç¾ã‚’å¾©å…ƒ
                    for i, emotion in enumerate(emotional_parts):
                        segment = segment.replace(f"<EMO{i}>", emotion)
                    segments.append(segment)
                    start = pos
        
        # æ®‹ã‚Šã‚’å‡¦ç†
        if start < len(text_cleaned):
            remaining = text_cleaned[start:].strip()
            if remaining:
                # æ„Ÿæƒ…è¡¨ç¾ã‚’å¾©å…ƒ
                for i, emotion in enumerate(emotional_parts):
                    remaining = remaining.replace(f"<EMO{i}>", emotion)
                segments.append(remaining)
        
        return segments if segments else [text]

class VOICEVOXSRTGenerator:
    """VOICEVOX SRTã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆæ§‹é€ ä¿®æ­£ç‰ˆï¼‰"""
    
    def __init__(self):
        self.splitter = AdvancedSegmentSplitter()
    
    def parse_vvproj(self, vvproj_path: str) -> Dict[str, Any]:
        """VVPROJãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£ã—ã„æ§‹é€ ã§è§£æ"""
        with open(vvproj_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        talk = data.get('talk', {})
        
        # æ­£ã—ã„æ§‹é€ ã§ã®å–å¾—
        audio_keys = talk.get('audioKeys', [])      # ãƒªã‚¹ãƒˆï¼ˆé †åºé‡è¦ï¼‰
        audio_items = talk.get('audioItems', {})    # è¾æ›¸ï¼ˆã‚­ãƒ¼ã§ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        
        print(f"ğŸ® NextStage Gaming - Processing VVPROJ file: {Path(vvproj_path).name}")
        print(f"ğŸ“‹ audioKeys: {len(audio_keys)} items (é †åºãƒªã‚¹ãƒˆ)")
        print(f"ğŸ“ audioItems: {len(audio_items)} items (è¾æ›¸)")
        
        # æ§‹é€ æ¤œè¨¼
        if not isinstance(audio_keys, list):
            raise ValueError(f"audioKeys should be list, got {type(audio_keys)}")
        if not isinstance(audio_items, dict):
            raise ValueError(f"audioItems should be dict, got {type(audio_items)}")
        
        # ã‚­ãƒ¼ã®ä¸€è‡´ç¢ºèª
        keys_set = set(audio_keys)
        items_set = set(audio_items.keys())
        if keys_set != items_set:
            print(f"âš ï¸  Warning: Keys mismatch - audioKeys: {len(keys_set)}, audioItems: {len(items_set)}")
        
        return {
            'audio_keys': audio_keys,
            'audio_items': audio_items
        }
    
    def convert_vvproj_to_audioquery(self, query_data: Dict[str, Any]) -> AudioQuery:
        """VVPROJã®queryãƒ‡ãƒ¼ã‚¿ã‚’AudioQueryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
        accent_phrases = []
        
        for phrase_data in query_data.get('accentPhrases', []):
            moras = []
            for mora_data in phrase_data.get('moras', []):
                mora = Mora(
                    text=mora_data.get('text', ''),
                    consonant=mora_data.get('consonant'),
                    consonant_length=mora_data.get('consonantLength'),
                    vowel=mora_data.get('vowel', ''),
                    vowel_length=mora_data.get('vowelLength', 0.0),
                    pitch=mora_data.get('pitch', 0.0)
                )
                moras.append(mora)
            
            pause_mora = None
            if 'pauseMora' in phrase_data and phrase_data['pauseMora']:
                pause_data = phrase_data['pauseMora']
                pause_mora = Mora(
                    text=pause_data.get('text', ''),
                    vowel=pause_data.get('vowel', 'pau'),
                    vowel_length=pause_data.get('vowelLength', 0.0),
                    pitch=0.0
                )
            
            accent_phrase = AccentPhrase(
                moras=moras,
                accent=phrase_data.get('accent', 1),
                pause_mora=pause_mora,
                is_interrogative=phrase_data.get('isInterrogative', False)
            )
            accent_phrases.append(accent_phrase)
        
        return AudioQuery(
            accent_phrases=accent_phrases,
            speedScale=query_data.get('speedScale', 1.0),
            pitchScale=query_data.get('pitchScale', 0.0),
            intonationScale=query_data.get('intonationScale', 1.0),
            volumeScale=query_data.get('volumeScale', 1.0),
            prePhonemeLength=query_data.get('prePhonemeLength', 0.1),
            postPhonemeLength=query_data.get('postPhonemeLength', 0.1),
            pauseLength=query_data.get('pauseLength'),
            pauseLengthScale=query_data.get('pauseLengthScale', 1.0),
            outputSamplingRate=query_data.get('outputSamplingRate', DEFAULT_SAMPLING_RATE),
            outputStereo=query_data.get('outputStereo', False),
            kana=query_data.get('kana')
        )
    
    def format_time(self, seconds: float) -> str:
        """ç§’ã‚’ SRT ã‚¿ã‚¤ãƒ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = seconds % 60
        return f"{hours:02d}:{minutes:02d}:{secs:06.3f}".replace('.', ',')
    
    def generate_srt(self, vvproj_path: str, output_path: Optional[str] = None) -> str:
        """VVPROJãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆæ§‹é€ ä¿®æ­£ç‰ˆï¼‰"""
        # VVPROJãƒ•ã‚¡ã‚¤ãƒ«è§£æ
        parsed_data = self.parse_vvproj(vvproj_path)
        audio_keys = parsed_data['audio_keys']
        audio_items = parsed_data['audio_items']
        
        srt_entries = []
        current_time = 0.0  # é€£ç¶šæ™‚é–“è»¸
        
        print(f"ğŸ“ å‡¦ç†å¯¾è±¡: {len(audio_keys)} items")
        
        for i, key in enumerate(audio_keys):  # audioKeysã®é †åºã§å‡¦ç†
            if key not in audio_items:
                print(f"âš ï¸  Warning: Key {key} not found in audioItems")
                continue
            
            item = audio_items[key]
            text = item.get('text', '')
            
            print(f"\nğŸ¯ Processing item {i+1}/{len(audio_keys)}")
            print(f"ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
            
            # AudioQueryæ§‹ç¯‰
            query_data = item.get('query', {})
            audio_query = self.convert_vvproj_to_audioquery(query_data)
            
            # æ­£ç¢ºãªéŸ³å£°æ™‚é–“è¨ˆç®—
            duration = VOICEVOXOfficialCalculator.calculate_accurate_duration(audio_query)
            print(f"â±ï¸  ç·èª­ã¿ä¸Šã’æ™‚é–“: {duration:.3f}ç§’")
            
            # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
            segments = self.splitter.split_text_smart(text, MAX_CHARS)
            print(f"âœ‚ï¸  åˆ†å‰²çµæœ: {len(segments)} segments")
            
            # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«æ™‚é–“ã‚’é…åˆ†
            if len(segments) == 1:
                # åˆ†å‰²ãªã—ã®å ´åˆ
                start_time = current_time
                end_time = current_time + duration
                
                entry = SRTEntry(
                    index=len(srt_entries) + 1,
                    start_time=self.format_time(start_time),
                    end_time=self.format_time(end_time),
                    text=segments[0]
                )
                srt_entries.append(entry)
                print(f"  ğŸ“ Segment 1: {segments[0][:50]}...")
                print(f"    â±ï¸  æ™‚é–“: {duration:.3f}ç§’")
            else:
                # è¤‡æ•°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å ´åˆã€æ–‡å­—æ•°æ¯”ã§æ™‚é–“é…åˆ†
                total_chars = sum(len(s) for s in segments)
                segment_start = current_time
                
                for j, segment in enumerate(segments):
                    char_ratio = len(segment) / total_chars
                    segment_duration = duration * char_ratio
                    segment_end = segment_start + segment_duration
                    
                    entry = SRTEntry(
                        index=len(srt_entries) + 1,
                        start_time=self.format_time(segment_start),
                        end_time=self.format_time(segment_end),
                        text=segment
                    )
                    srt_entries.append(entry)
                    print(f"  ğŸ“ Segment {j+1}: {segment}")
                    print(f"    â±ï¸  æ™‚é–“: {segment_duration:.3f}ç§’")
                    
                    segment_start = segment_end
            
            # æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã®ãŸã‚ã«æ™‚é–“ã‚’é€²ã‚ã‚‹ï¼ˆé€£ç¶šæ™‚é–“è»¸ï¼‰
            current_time += duration
        
        # SRTãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        srt_content = ""
        for entry in srt_entries:
            srt_content += f"{entry.index}\n"
            srt_content += f"{entry.start_time} --> {entry.end_time}\n"
            srt_content += f"{entry.text}\n\n"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_path is None:
            vvproj_file = Path(vvproj_path)
            output_path = vvproj_file.parent / f"{vvproj_file.stem}_fixed.srt"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        print(f"\nâœ… SRTãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {output_path}")
        print(f"ğŸ“Š ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(srt_entries)}")
        print(f"â±ï¸  ç·æ™‚é–“: {current_time:.3f}ç§’")
        
        return str(output_path)

def main():
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python voicevox_srt_generator_fixed_structure.py <vvproj_file>")
        sys.exit(1)
    
    vvproj_path = sys.argv[1]
    
    if not Path(vvproj_path).exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vvproj_path}")
        sys.exit(1)
    
    generator = VOICEVOXSRTGenerator()
    try:
        output_path = generator.generate_srt(vvproj_path)
        print(f"\nğŸ‰ ç”ŸæˆæˆåŠŸ: {output_path}")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()