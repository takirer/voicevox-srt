#!/usr/bin/env python3
"""
gen-srt-from-vvproj.py

VOICEVOX-SRT å…¬å¼ä»•æ§˜æº–æ‹ ç‰ˆ
VOICEVOXã§ç”Ÿæˆã—ãŸvvprojãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Œå…¨åŒæœŸã™ã‚‹SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›

ä¸»è¦æ©Ÿèƒ½:
- VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªæ™‚é–“è¨ˆç®—
- æ„Ÿæƒ…è¡¨ç¾ï¼ˆå¥èª­ç‚¹ãƒ»æ„Ÿå˜†è©ç¹°ã‚Šè¿”ã—ï¼‰ã®è‡ªç„¶ãªä¿æŒ
- fugashiï¼ˆMeCabï¼‰ã«ã‚ˆã‚‹è‡ªç„¶ãªæ—¥æœ¬èªè¡Œåˆ†å‰²
- MAX_CHARS/MAX_LINESè¦ä»¶ã¸ã®æº–æ‹ 

Author: AI Assistant (based on yKesamaru/voicevox-srt + VOICEVOX official implementation)
For: NextStage Gaming ãƒãƒ£ãƒ³ãƒãƒ« Street Fighter 6å®Ÿæ³å‹•ç”»ç·¨é›†åŠ¹ç‡åŒ–
"""

import json
import math
import re
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import numpy as np

# fugashi (MeCab wrapper) for natural Japanese segmentation
try:
    from fugashi import GenericTagger
    MECAB_AVAILABLE = True
except ImportError:
    print("Warning: fugashi not available. Natural segmentation will be limited.")
    MECAB_AVAILABLE = False

# VOICEVOX official constants
FRAMERATE = 93.75  # 24000 / 256 [frame/sec] - VOICEVOX official framerate
DEFAULT_SAMPLING_RATE = 24000

# Configuration constants
MAX_CHARS = 26  # åŸºæœ¬æ–‡å­—æ•°åˆ¶é™ï¼ˆæ„Ÿæƒ…è¡¨ç¾ã¯ä¾‹å¤–ï¼‰
MAX_LINES = 2   # å³å¯†ãªè¡Œæ•°åˆ¶é™
UPSPEAK_LENGTH = 0.15
UPSPEAK_PITCH_ADD = 0.3
UPSPEAK_PITCH_MAX = 6.5

@dataclass
class Mora:
    """VOICEVOXå…¬å¼æº–æ‹ ã®Moraãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    text: str
    consonant: Optional[str] = None
    consonant_length: Optional[float] = None
    vowel: str = ""
    vowel_length: float = 0.0
    pitch: float = 0.0

@dataclass
class AccentPhrase:
    """VOICEVOXå…¬å¼æº–æ‹ ã®AccentPhraseãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    moras: List[Mora] = field(default_factory=list)
    accent: int = 1
    pause_mora: Optional[Mora] = None
    is_interrogative: bool = False

@dataclass
class AudioQuery:
    """VOICEVOXå…¬å¼æº–æ‹ ã®AudioQueryãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    accent_phrases: List[AccentPhrase] = field(default_factory=list)
    speedScale: float = 1.0
    pitchScale: float = 0.0
    intonationScale: float = 1.0
    volumeScale: float = 1.0
    prePhonemeLength: float = 0.1
    postPhonemeLength: float = 0.1
    pauseLength: Optional[float] = None
    pauseLengthScale: float = 1.0
    outputSamplingRate: int = DEFAULT_SAMPLING_RATE
    outputStereo: bool = False
    kana: Optional[str] = None

@dataclass
class SRTEntry:
    """SRTå­—å¹•ã‚¨ãƒ³ãƒˆãƒª"""
    index: int
    start_time: str
    end_time: str
    text: str

class VOICEVOXOfficialCalculator:
    """VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªæ™‚é–“è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def _to_frame(sec: float) -> int:
        """
        VOICEVOXå…¬å¼ã®ç§’â†’ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
        NOTE: roundã¯å¶æ•°ä¸¸ã‚ã€‚ç§»æ¤æ™‚ã«å–æ‰±ã„æ³¨æ„ã€‚
        """
        sec_rounded = np.round(sec * FRAMERATE)
        return int(sec_rounded)
    
    @staticmethod
    def _generate_silence_mora(length: float) -> Mora:
        """éŸ³ã®é•·ã•ã‚’æŒ‡å®šã—ã¦ç„¡éŸ³ãƒ¢ãƒ¼ãƒ©ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
        return Mora(text="ã€€", vowel="sil", vowel_length=length, pitch=0.0)
    
    @staticmethod
    def _apply_prepost_silence(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤å‰å¾Œç„¡éŸ³ã‚’ä»˜åŠ ã™ã‚‹"""
        pre_silence_moras = [VOICEVOXOfficialCalculator._generate_silence_mora(query.prePhonemeLength)]
        post_silence_moras = [VOICEVOXOfficialCalculator._generate_silence_mora(query.postPhonemeLength)]
        return pre_silence_moras + moras + post_silence_moras
    
    @staticmethod
    def _apply_pause_length(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤ç„¡éŸ³æ™‚é–“ã‚’é©ç”¨ã™ã‚‹"""
        if query.pauseLength is not None:
            for mora in moras:
                if mora.vowel == "pau":
                    mora.vowel_length = query.pauseLength
        return moras
    
    @staticmethod
    def _apply_pause_length_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤ç„¡éŸ³æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            if mora.vowel == "pau":
                mora.vowel_length *= query.pauseLengthScale
        return moras
    
    @staticmethod
    def _apply_speed_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤è©±é€Ÿã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            mora.vowel_length /= query.speedScale
            if mora.consonant_length:
                mora.consonant_length /= query.speedScale
        return moras
    
    @staticmethod
    def _apply_pitch_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤éŸ³é«˜ã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        for mora in moras:
            mora.pitch *= 2**query.pitchScale
        return moras
    
    @staticmethod
    def _apply_intonation_scale(moras: List[Mora], query: AudioQuery) -> List[Mora]:
        """ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã¸éŸ³å£°åˆæˆç”¨ã®ã‚¯ã‚¨ãƒªãŒã‚‚ã¤æŠ‘æšã‚¹ã‚±ãƒ¼ãƒ«ã‚’é©ç”¨ã™ã‚‹"""
        # æœ‰å£°éŸ³ç´  (f0>0) ã®å¹³å‡å€¤ã«å¯¾ã™ã‚‹ä¹–é›¢åº¦ã‚’ã‚¹ã‚±ãƒ¼ãƒ«
        voiced = [mora for mora in moras if mora.pitch > 0]
        if voiced:
            mean_f0 = np.mean([mora.pitch for mora in voiced])
            if not math.isnan(mean_f0):
                for mora in voiced:
                    mora.pitch = (mora.pitch - mean_f0) * query.intonationScale + mean_f0
        return moras
    
    @staticmethod
    def _count_frame_per_unit(moras: List[Mora]) -> Tuple[np.ndarray, np.ndarray]:
        """
        éŸ³ç´ ã‚ãŸã‚Šãƒ»ãƒ¢ãƒ¼ãƒ©ã‚ãŸã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ é•·ã‚’ç®—å‡ºã™ã‚‹
        
        Returns:
            frame_per_phoneme: éŸ³ç´ ã‚ãŸã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ é•·
            frame_per_mora: ãƒ¢ãƒ¼ãƒ©ã‚ãŸã‚Šã®ãƒ•ãƒ¬ãƒ¼ãƒ é•·
        """
        frame_per_phoneme = []
        frame_per_mora = []
        
        for mora in moras:
            vowel_frames = VOICEVOXOfficialCalculator._to_frame(mora.vowel_length)
            consonant_frames = (
                VOICEVOXOfficialCalculator._to_frame(mora.consonant_length) 
                if mora.consonant_length is not None else 0
            )
            mora_frames = vowel_frames + consonant_frames
            
            if mora.consonant:
                frame_per_phoneme.append(consonant_frames)
            frame_per_phoneme.append(vowel_frames)
            frame_per_mora.append(mora_frames)
        
        return np.array(frame_per_phoneme, dtype=np.int64), np.array(frame_per_mora, dtype=np.int64)
    
    @staticmethod
    def calculate_accurate_duration(query: AudioQuery) -> float:
        """
        VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªéŸ³å£°æ™‚é–“è¨ˆç®—
        _query_to_decoder_featureã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å†ç¾
        """
        # ã‚¢ã‚¯ã‚»ãƒ³ãƒˆå¥ç³»åˆ—ã‹ã‚‰ãƒ¢ãƒ¼ãƒ©ç³»åˆ—ã‚’æŠ½å‡º
        moras = []
        for accent_phrase in query.accent_phrases:
            moras.extend(accent_phrase.moras)
            if accent_phrase.pause_mora:
                moras.append(accent_phrase.pause_mora)
        
        # è¨­å®šã‚’é©ç”¨ã™ã‚‹é †åºãŒé‡è¦ï¼ˆå…¬å¼å®Ÿè£…ã¨åŒé †åºï¼‰
        moras = VOICEVOXOfficialCalculator._apply_prepost_silence(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_pause_length(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_pause_length_scale(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_speed_scale(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_pitch_scale(moras, query)
        moras = VOICEVOXOfficialCalculator._apply_intonation_scale(moras, query)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã§ã®è¨ˆç®—
        _, frame_per_mora = VOICEVOXOfficialCalculator._count_frame_per_unit(moras)
        
        # ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’è¨ˆç®—
        total_frames = np.sum(frame_per_mora)
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ç§’ã«å¤‰æ›
        total_seconds = total_frames / FRAMERATE
        
        return total_seconds

class EmotionalExpressionHandler:
    """æ„Ÿæƒ…è¡¨ç¾ï¼ˆå¥èª­ç‚¹ãƒ»æ„Ÿå˜†è©ç¹°ã‚Šè¿”ã—ï¼‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    # æ„Ÿæƒ…è¡¨ç¾ã¨ã—ã¦èªè­˜ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
    EMOTION_PATTERN = r'([ã€‚ï¼ï¼Ÿã€â€¦ãƒ»ãƒ¼ï½]{2,})$'
    SINGLE_PUNCT_PATTERN = r'[ã€‚ï¼ï¼Ÿã€â€¦ãƒ»ãƒ¼ï½]+$'
    
    @classmethod
    def analyze_emotional_expression(cls, text: str) -> Dict[str, Any]:
        """æ„Ÿæƒ…è¡¨ç¾ã‚’è©³ç´°åˆ†æ"""
        match = re.search(cls.EMOTION_PATTERN, text)
        
        if match:
            base_text = text[:match.start()]
            emotion_part = match.group(1)
            
            return {
                'has_emotion': True,
                'base_text': base_text,
                'emotion_part': emotion_part,
                'base_length': len(base_text),
                'emotion_length': len(emotion_part),
                'total_length': len(text)
            }
        
        return {
            'has_emotion': False,
            'base_text': text,
            'emotion_part': '',
            'base_length': len(text),
            'emotion_length': 0,
            'total_length': len(text)
        }
    
    @classmethod
    def is_chars_allowed_with_emotion(cls, text: str, max_chars: int = MAX_CHARS) -> bool:
        """æ„Ÿæƒ…è¡¨ç¾ã‚’è€ƒæ…®ã—ãŸæ–‡å­—æ•°åˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        if len(text) <= max_chars:
            return True
        
        analysis = cls.analyze_emotional_expression(text)
        
        # æ„Ÿæƒ…è¡¨ç¾ãŒã‚ã‚‹å ´åˆã€åŸºæœ¬æ–‡å­—éƒ¨åˆ†ã®ã¿ã‚’åˆ¶é™å¯¾è±¡ã¨ã™ã‚‹
        if analysis['has_emotion']:
            return analysis['base_length'] <= max_chars
        
        return False
    
    @classmethod
    def is_meaningless_punctuation(cls, text: str) -> bool:
        """æ„å‘³ã®ãªã„å¥èª­ç‚¹ã®ã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ãƒã‚§ãƒƒã‚¯"""
        stripped = text.strip()
        if not stripped:
            return True
        
        # 1æ–‡å­—ä»¥ä¸‹ã®å¥èª­ç‚¹ã®ã¿ã¯æ„å‘³ãŒãªã„
        if len(stripped) <= 1 and re.match(cls.SINGLE_PUNCT_PATTERN, stripped):
            return True
        
        return False

class NaturalSegmentator:
    """fugashiã«ã‚ˆã‚‹è‡ªç„¶ãªæ—¥æœ¬èªè¡Œåˆ†å‰²ã‚¯ãƒ©ã‚¹ï¼ˆæ„Ÿæƒ…è¡¨ç¾å¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self):
        if MECAB_AVAILABLE:
            try:
                self.tagger = GenericTagger("-r /opt/homebrew/etc/mecabrc -d /opt/homebrew/lib/mecab/dic/ipadic")
            except Exception as e:
                print(f"Warning: Failed to initialize MeCab: {e}")
                self.tagger = None
        else:
            self.tagger = None
        
        self.emotion_handler = EmotionalExpressionHandler()
    
    def segment_text(self, text: str, max_chars: int = MAX_CHARS, max_lines: int = MAX_LINES) -> List[str]:
        """
        æ„Ÿæƒ…è¡¨ç¾ã‚’è€ƒæ…®ã—ãŸè‡ªç„¶ãªæ—¥æœ¬èªåˆ†å‰²
        """
        if not text.strip():
            return []
        
        # åŸºæœ¬çš„ãªåˆ†å‰²å€™è£œã‚’å–å¾—
        segments = self._get_natural_break_points(text)
        
        # æ„å‘³ã®ãªã„å¥èª­ç‚¹ã‚¨ãƒ³ãƒˆãƒªã‚’é™¤å¤–
        meaningful_segments = []
        for segment in segments:
            if not self.emotion_handler.is_meaningless_punctuation(segment):
                meaningful_segments.append(segment)
        
        if not meaningful_segments:
            return []
        
        # MAX_CHARS/MAX_LINESåˆ¶é™ã«æº–æ‹ ã—ãŸåˆ†å‰²ï¼ˆæ„Ÿæƒ…è¡¨ç¾è€ƒæ…®ï¼‰
        result = self._enforce_limits_with_emotion(meaningful_segments, max_chars, max_lines)
        
        return result
    
    def _get_natural_break_points(self, text: str) -> List[str]:
        """è‡ªç„¶ãªåˆ†å‰²å€™è£œã‚’å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        if not self.tagger:
            return self._simple_segmentation(text)
        
        # ã‚ˆã‚Šè‡ªç„¶ãªåˆ†å‰²ã®ãŸã‚ã€å¥èª­ç‚¹ã§ã®åˆ†å‰²ã‚’æ”¹è‰¯
        # å¥èª­ç‚¹ã¯å‰ã®æ–‡ç« ã¨çµ±åˆã™ã‚‹ã“ã¨ã‚’åŸºæœ¬ã¨ã™ã‚‹
        sentences = []
        current = ""
        
        i = 0
        while i < len(text):
            char = text[i]
            current += char
            
            # å¥èª­ç‚¹ã«é­é‡ã—ãŸå ´åˆ
            if char in "ã€‚ï¼ï¼Ÿ":
                # é€£ç¶šã™ã‚‹å¥èª­ç‚¹ã‚‚å«ã‚ã‚‹
                while i + 1 < len(text) and text[i + 1] in "ã€‚ï¼ï¼Ÿã€â€¦ãƒ»ãƒ¼ï½":
                    i += 1
                    current += text[i]
                
                # ç¾åœ¨ã®æ–‡ã‚’ç¢ºå®š
                if current.strip():
                    sentences.append(current.strip())
                    current = ""
            
            i += 1
        
        # æ®‹ã‚Šã®éƒ¨åˆ†ãŒã‚ã‚Œã°è¿½åŠ 
        if current.strip():
            sentences.append(current.strip())
        
        # é•·ã™ãã‚‹ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã‚’MeCabã§ç´°åˆ†å‰²
        refined_sentences = []
        for sentence in sentences:
            if self.emotion_handler.is_chars_allowed_with_emotion(sentence, MAX_CHARS):
                refined_sentences.append(sentence)
            else:
                refined_sentences.extend(self._mecab_segmentation(sentence))
        
        return refined_sentences
    
    def _mecab_segmentation(self, text: str) -> List[str]:
        """MeCabã«ã‚ˆã‚‹å½¢æ…‹ç´ è§£æåˆ†å‰²ï¼ˆæ„Ÿæƒ…è¡¨ç¾è€ƒæ…®ï¼‰"""
        try:
            # æ„Ÿæƒ…è¡¨ç¾éƒ¨åˆ†ã‚’ä¿è­·
            emotion_analysis = self.emotion_handler.analyze_emotional_expression(text)
            
            if emotion_analysis['has_emotion']:
                base_text = emotion_analysis['base_text']
                emotion_part = emotion_analysis['emotion_part']
                
                # åŸºæœ¬éƒ¨åˆ†ã®ã¿ã‚’åˆ†å‰²
                words = []
                for word in self.tagger(base_text):
                    words.append(str(word.surface))
                
                # å˜èªã‚’é©åˆ‡ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                segments = []
                current = ""
                
                for word in words:
                    if len(current + word) <= MAX_CHARS:
                        current += word
                    else:
                        if current:
                            segments.append(current)
                        current = word
                
                # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«æ„Ÿæƒ…è¡¨ç¾ã‚’è¿½åŠ 
                if current:
                    current += emotion_part
                    segments.append(current)
                elif segments:
                    segments[-1] += emotion_part
                
                return segments
            else:
                # é€šå¸¸ã®åˆ†å‰²å‡¦ç†
                words = []
                for word in self.tagger(text):
                    words.append(str(word.surface))
                
                segments = []
                current = ""
                
                for word in words:
                    if len(current + word) <= MAX_CHARS:
                        current += word
                    else:
                        if current:
                            segments.append(current)
                        current = word
                
                if current:
                    segments.append(current)
                
                return segments
                
        except Exception:
            return self._simple_segmentation(text)
    
    def _simple_segmentation(self, text: str) -> List[str]:
        """MeCabåˆ©ç”¨ä¸å¯æ™‚ã®ç°¡æ˜“åˆ†å‰²ï¼ˆæ„Ÿæƒ…è¡¨ç¾è€ƒæ…®ï¼‰"""
        segments = []
        current = ""
        
        for char in text:
            test_text = current + char
            if self.emotion_handler.is_chars_allowed_with_emotion(test_text, MAX_CHARS):
                current = test_text
            else:
                if current:
                    segments.append(current)
                current = char
        
        if current:
            segments.append(current)
        
        return segments
    
    def _enforce_limits_with_emotion(self, segments: List[str], max_chars: int, max_lines: int) -> List[str]:
        """MAX_CHARS/MAX_LINESåˆ¶é™ã®å³å¯†ãªå®Ÿè£…ï¼ˆæ„Ÿæƒ…è¡¨ç¾è€ƒæ…®ï¼‰"""
        if not segments:
            return []
        
        result = []
        current_lines = []
        
        for segment in segments:
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯å¼·åˆ¶åˆ†å‰²
            if not self.emotion_handler.is_chars_allowed_with_emotion(segment, max_chars):
                segment_parts = self._force_split_with_emotion(segment, max_chars)
            else:
                segment_parts = [segment]
            
            for part in segment_parts:
                # ç¾åœ¨ã®è¡Œã«è¿½åŠ å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                if len(current_lines) < max_lines:
                    if not current_lines:
                        current_lines.append(part)
                    else:
                        # æœ€å¾Œã®è¡Œã«è¿½åŠ å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                        last_line = current_lines[-1]
                        combined = last_line + part
                        if self.emotion_handler.is_chars_allowed_with_emotion(combined, max_chars):
                            current_lines[-1] = combined
                        else:
                            # æ–°ã—ã„è¡Œã¨ã—ã¦è¿½åŠ 
                            if len(current_lines) < max_lines:
                                current_lines.append(part)
                            else:
                                # è¡Œæ•°åˆ¶é™ã«é”ã—ãŸå ´åˆã€ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ç¢ºå®š
                                result.append('\n'.join(current_lines))
                                current_lines = [part]
                else:
                    # è¡Œæ•°åˆ¶é™ã«é”ã—ãŸå ´åˆã€ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ç¢ºå®š
                    result.append('\n'.join(current_lines))
                    current_lines = [part]
        
        # æ®‹ã‚Šã®è¡Œã‚’è¿½åŠ 
        if current_lines:
            result.append('\n'.join(current_lines))
        
        return result
    
    def _force_split_with_emotion(self, text: str, max_chars: int) -> List[str]:
        """æ–‡å­—æ•°åˆ¶é™ã‚’è¶…ãˆã‚‹å ´åˆã®å¼·åˆ¶åˆ†å‰²ï¼ˆæ„Ÿæƒ…è¡¨ç¾è€ƒæ…®ï¼‰"""
        emotion_analysis = self.emotion_handler.analyze_emotional_expression(text)
        
        if emotion_analysis['has_emotion']:
            # æ„Ÿæƒ…è¡¨ç¾ãŒã‚ã‚‹å ´åˆã€åŸºæœ¬éƒ¨åˆ†ã‚’åˆ†å‰²ã—æœ€å¾Œã«æ„Ÿæƒ…è¡¨ç¾ã‚’ä»˜åŠ 
            base_text = emotion_analysis['base_text']
            emotion_part = emotion_analysis['emotion_part']
            
            base_parts = []
            current = ""
            
            for char in base_text:
                if len(current + char) <= max_chars:
                    current += char
                else:
                    if current:
                        base_parts.append(current)
                    current = char
            
            if current:
                current += emotion_part
                base_parts.append(current)
            elif base_parts:
                base_parts[-1] += emotion_part
            
            return base_parts
        else:
            # é€šå¸¸ã®å¼·åˆ¶åˆ†å‰²
            parts = []
            current = ""
            
            for char in text:
                if len(current + char) <= max_chars:
                    current += char
                else:
                    if current:
                        parts.append(current)
                    current = char
            
            if current:
                parts.append(current)
            
            return parts

class VOICEVOXSRTGenerator:
    """VOICEVOXå…¬å¼ä»•æ§˜æº–æ‹ ã®SRTã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ï¼ˆæ„Ÿæƒ…è¡¨ç¾å¯¾å¿œç‰ˆï¼‰"""
    
    def __init__(self):
        self.calculator = VOICEVOXOfficialCalculator()
        self.segmentator = NaturalSegmentator()
    
    def process_vvproj(self, vvproj_path: str, output_path: Optional[str] = None) -> str:
        """VVPROJãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        print(f"Processing VVPROJ file: {vvproj_path}")
        
        # VVPROJãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(vvproj_path, 'r', encoding='utf-8') as f:
            vvproj_data = json.load(f)
        
        # audioItemsã‚’å–å¾—
        audio_items = vvproj_data.get('talk', {}).get('audioItems', {})
        if not audio_items:
            raise ValueError("No audioItems found in VVPROJ file")
        
        # audioKeysã®é †åºã‚’å–å¾—
        audio_keys = vvproj_data.get('talk', {}).get('audioKeys', [])
        if not audio_keys:
            audio_keys = list(audio_items.keys())
        
        print(f"Found {len(audio_items)} audio items, {len(audio_keys)} audio keys")
        
        # SRTã‚¨ãƒ³ãƒˆãƒªã‚’ç”Ÿæˆ
        srt_entries = self._generate_srt_entries(audio_items, audio_keys)
        
        # SRTãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
        if output_path is None:
            vvproj_path_obj = Path(vvproj_path)
            output_path = vvproj_path_obj.with_suffix('.srt')
        
        srt_content = self._write_srt_file(srt_entries, output_path)
        
        print(f"SRT file generated: {output_path}")
        print(f"Total entries: {len(srt_entries)}")
        
        return srt_content
    
    def _vvproj_to_audio_query(self, audio_item: Dict[str, Any]) -> AudioQuery:
        """VVPROJã®audioItemã‚’AudioQueryã«å¤‰æ›"""
        query_data = audio_item.get('query', {})
        
        # AccentPhraseã‚’å¤‰æ›
        accent_phrases = []
        for phrase_data in query_data.get('accentPhrases', []):
            # Moraã‚’å¤‰æ›
            moras = []
            for mora_data in phrase_data.get('moras', []):
                mora = Mora(
                    text=mora_data.get('text', ''),
                    consonant=mora_data.get('consonant'),
                    consonant_length=mora_data.get('consonantLength'),
                    vowel=mora_data.get('vowel', ''),
                    vowel_length=mora_data.get('vowelLength', 0.0),
                    pitch=mora_data.get('pitch', 0.0)
                )
                moras.append(mora)
            
            # PauseMoraã‚’å¤‰æ›
            pause_mora = None
            if phrase_data.get('pauseMora'):
                pause_data = phrase_data['pauseMora']
                pause_mora = Mora(
                    text=pause_data.get('text', ''),
                    consonant=pause_data.get('consonant'),
                    consonant_length=pause_data.get('consonantLength'),
                    vowel=pause_data.get('vowel', ''),
                    vowel_length=pause_data.get('vowelLength', 0.0),
                    pitch=pause_data.get('pitch', 0.0)
                )
            
            accent_phrase = AccentPhrase(
                moras=moras,
                accent=phrase_data.get('accent', 1),
                pause_mora=pause_mora,
                is_interrogative=phrase_data.get('isInterrogative', False)
            )
            accent_phrases.append(accent_phrase)
        
        # AudioQueryã‚’æ§‹ç¯‰
        audio_query = AudioQuery(
            accent_phrases=accent_phrases,
            speedScale=query_data.get('speedScale', 1.0),
            pitchScale=query_data.get('pitchScale', 0.0),
            intonationScale=query_data.get('intonationScale', 1.0),
            volumeScale=query_data.get('volumeScale', 1.0),
            prePhonemeLength=query_data.get('prePhonemeLength', 0.1),
            postPhonemeLength=query_data.get('postPhonemeLength', 0.1),
            pauseLength=query_data.get('pauseLength'),
            pauseLengthScale=query_data.get('pauseLengthScale', 1.0),
            outputSamplingRate=query_data.get('outputSamplingRate', DEFAULT_SAMPLING_RATE),
            outputStereo=query_data.get('outputStereo', False),
            kana=query_data.get('kana')
        )
        
        return audio_query
    
    def _generate_srt_entries(self, audio_items: Dict[str, Any], audio_keys: List[str]) -> List[SRTEntry]:
        """SRTã‚¨ãƒ³ãƒˆãƒªã‚’ç”Ÿæˆï¼ˆæ„Ÿæƒ…è¡¨ç¾å¯¾å¿œï¼‰"""
        srt_entries = []
        current_time = 0.0
        
        for i, audio_key in enumerate(audio_keys):
            if audio_key not in audio_items:
                print(f"Warning: Audio key {audio_key} not found in audioItems")
                continue
            
            audio_item = audio_items[audio_key]
            text = audio_item.get('text', '')
            
            if not text.strip():
                continue
            
            # AudioQueryã«å¤‰æ›
            audio_query = self._vvproj_to_audio_query(audio_item)
            
            # VOICEVOXå…¬å¼å®Ÿè£…ã«åŸºã¥ãæ­£ç¢ºãªæ™‚é–“è¨ˆç®—
            duration = self.calculator.calculate_accurate_duration(audio_query)
            
            # æ„Ÿæƒ…è¡¨ç¾ã‚’è€ƒæ…®ã—ãŸè‡ªç„¶ãªåˆ†å‰²ã‚’é©ç”¨
            text_segments = self.segmentator.segment_text(text, MAX_CHARS, MAX_LINES)
            
            if not text_segments:
                current_time += duration
                continue
            
            # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ™‚é–“ã‚’å‡ç­‰åˆ†å‰²
            segment_duration = duration / len(text_segments) if text_segments else duration
            
            for j, segment_text in enumerate(text_segments):
                if not segment_text.strip():
                    continue
                
                start_time = current_time + (j * segment_duration)
                end_time = start_time + segment_duration
                
                srt_entry = SRTEntry(
                    index=len(srt_entries) + 1,
                    start_time=self._seconds_to_srt_time(start_time),
                    end_time=self._seconds_to_srt_time(end_time),
                    text=segment_text.strip()
                )
                srt_entries.append(srt_entry)
            
            current_time += duration
        
        return srt_entries
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """ç§’ã‚’SRTæ™‚é–“å½¢å¼ã«å¤‰æ›"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def _write_srt_file(self, srt_entries: List[SRTEntry], output_path: str) -> str:
        """SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›"""
        srt_content = []
        
        for entry in srt_entries:
            srt_content.append(str(entry.index))
            srt_content.append(f"{entry.start_time} --> {entry.end_time}")
            srt_content.append(entry.text)
            srt_content.append("")  # ç©ºè¡Œ
        
        content = '\n'.join(srt_content)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        return content

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    if len(sys.argv) < 2:
        print("Usage: python gen-srt-from-vvproj.py <vvproj_file> [output_srt_file]")
        sys.exit(1)
    
    vvproj_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None
    
    if not Path(vvproj_path).exists():
        print(f"Error: VVPROJ file not found: {vvproj_path}")
        sys.exit(1)
    
    try:
        generator = VOICEVOXSRTGenerator()
        generator.process_vvproj(vvproj_path, output_path)
        print("âœ… NextStage Gaming ãƒãƒ£ãƒ³ãƒãƒ«ç”¨SRTç”Ÿæˆå®Œäº†ï¼")
        print("ğŸ® Street Fighter 6å®Ÿæ³å‹•ç”»ç·¨é›†ã®åŠ¹ç‡åŒ–ã‚’å®Ÿç¾ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()